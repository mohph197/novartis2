{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d788fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efa971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "from src.preprocess import general_preprocessing, train_preprocessing, build_bucket_dataset\n",
    "from src.train import split_train_test, split_true, split_scenario, split_k_fold, split_bucket_train_test, split_buckets\n",
    "from src.model import fit_model, predict, fit_model_cv\n",
    "from src.bucket_classifier import fit_bucket_classifier, predict_bucket1_proba, predict_probas\n",
    "from src.metrics import compute_metric1, compute_metric2\n",
    "from src.plot import plot_interactive_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a13d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_aux = general_preprocessing(\n",
    "    pd.read_csv('data/train/df_volume_train.csv'),\n",
    "    pd.read_csv('data/train/df_generics_train.csv'),\n",
    "    pd.read_csv('data/train/df_medicine_info_train.csv'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80872d9f",
   "metadata": {},
   "source": [
    "## Classifier Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca72d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bucket_s1 = build_bucket_dataset(df, df_aux, \"s1\")\n",
    "df_bucket_s2 = build_bucket_dataset(df, df_aux, \"s2\")\n",
    "s1_roc_auc_scores = []\n",
    "s1_f1_scores = []\n",
    "for train_bucket_s1_df, eval_bucket_s1_df, _ in split_k_fold(df_bucket_s1):\n",
    "    bucket_model_s1 = fit_bucket_classifier(train_bucket_s1_df, \"s1\", verbose=False)\n",
    "    proba_bucket_s1 = predict_bucket1_proba(bucket_model_s1, eval_bucket_s1_df, \"s1\")\n",
    "    pred_bucket_s1 = (proba_bucket_s1 > 0.5).astype(int)\n",
    "\n",
    "    s1_roc_auc_score = roc_auc_score(eval_bucket_s1_df[\"label\"].values, proba_bucket_s1)\n",
    "    s1_f1_score = f1_score(eval_bucket_s1_df[\"label\"].values, pred_bucket_s1)\n",
    "    s1_roc_auc_scores.append(s1_roc_auc_score)\n",
    "    s1_f1_scores.append(s1_f1_score)\n",
    "\n",
    "    # ths = np.linspace(0.1, 0.5, 100)\n",
    "    # best = None\n",
    "    # for t in ths:\n",
    "    #     pred_bucket_s1 = (proba_bucket_s1 > t).astype(int)\n",
    "    #     scr = f1_score(eval_bucket_s1_df[\"label\"].values, pred_bucket_s1)\n",
    "    #     if best is None or scr > best:\n",
    "    #         best = scr\n",
    "    #         best_t = t\n",
    "\n",
    "    print('--------------------')\n",
    "    print('Scenario 1 metrics:', s1_roc_auc_score, s1_f1_score)\n",
    "    # print('Best threshold:', best_t, '-> f1=', best)\n",
    "    break\n",
    "print('---------------------------------')\n",
    "print(\"Mean Scenario 1 metrics:\", np.mean(s1_roc_auc_scores), np.mean(s1_f1_scores))\n",
    "\n",
    "s2_roc_auc_scores = []\n",
    "s2_f1_scores = []\n",
    "for train_bucket_s2_df, eval_bucket_s2_df, _ in split_k_fold(df_bucket_s2):\n",
    "    bucket_model_s2 = fit_bucket_classifier(train_bucket_s2_df, \"s2\", verbose=False)\n",
    "    proba_bucket_s2 = predict_bucket1_proba(bucket_model_s2, eval_bucket_s2_df, \"s2\")\n",
    "    pred_bucket_s2 = (proba_bucket_s2 > 0.5).astype(int)\n",
    "\n",
    "    s2_roc_auc_score = roc_auc_score(eval_bucket_s2_df[\"label\"].values, proba_bucket_s2)\n",
    "    s2_f1_score = f1_score(eval_bucket_s2_df[\"label\"].values, pred_bucket_s2)\n",
    "    s2_roc_auc_scores.append(s2_roc_auc_score)\n",
    "    s2_f1_scores.append(s2_f1_score)\n",
    "\n",
    "    # ths = np.linspace(0.1, 0.5, 100)\n",
    "    # best = None\n",
    "    # for t in ths:\n",
    "    #     pred_bucket_s2 = (proba_bucket_s2 > t).astype(int)\n",
    "    #     scr = f1_score(eval_bucket_s2_df[\"label\"].values, pred_bucket_s2)\n",
    "    #     if best is None or scr > best:\n",
    "    #         best = scr\n",
    "    #         best_t = t\n",
    "\n",
    "    print('--------------------')\n",
    "    print(\"Scenario 2 metrics:\", s2_roc_auc_score, s2_f1_score)\n",
    "    # print(\"Best threshold:\", best_t, '-> f1=', best)\n",
    "    break\n",
    "print('---------------------------------')\n",
    "print(\"Mean Scenario 2 metrics:\", np.mean(s2_roc_auc_scores), np.mean(s2_f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_probs_vs_buckets(base_df: pd.DataFrame, p_oof: np.ndarray, label_col=\"label\", title=\"\"):\n",
    "#     dfp = base_df.copy().reset_index(drop=True)\n",
    "#     dfp[\"p_high\"] = p_oof\n",
    "#     dfp[\"bucket_true\"] = np.where(dfp[label_col] == 1, \"bucket1 (high erosion)\", \"bucket2 (low erosion)\")\n",
    "\n",
    "#     # ---------- 1) Histogram by bucket ----------\n",
    "#     plt.figure(figsize=(8,5))\n",
    "#     for b, sub in dfp.groupby(\"bucket_true\"):\n",
    "#         plt.hist(sub[\"p_high\"], bins=30, alpha=0.5, density=True, label=b)\n",
    "#     plt.xlabel(\"Predicted P(high erosion)\")\n",
    "#     plt.ylabel(\"Density\")\n",
    "#     plt.title(f\"Probability distributions by bucket {title}\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # ---------- 2) Boxplot by bucket ----------\n",
    "#     plt.figure(figsize=(6,5))\n",
    "#     data = [dfp.loc[dfp[\"bucket_true\"].str.contains(\"bucket1\"), \"p_high\"],\n",
    "#             dfp.loc[dfp[\"bucket_true\"].str.contains(\"bucket2\"), \"p_high\"]]\n",
    "#     plt.boxplot(data, tick_labels=[\"bucket1\", \"bucket2\"])\n",
    "#     plt.ylabel(\"Predicted P(high erosion)\")\n",
    "#     plt.title(f\"Boxplot of probabilities by bucket {title}\")\n",
    "#     plt.show()\n",
    "\n",
    "#     # ---------- 3) Bin plot: avg prob vs true rate ----------\n",
    "#     # (like a calibration curve, but also good for threshold intuition)\n",
    "#     dfp[\"bin\"] = pd.qcut(dfp[\"p_high\"], q=10, duplicates=\"drop\")\n",
    "#     bin_stats = dfp.groupby(\"bin\", observed=True).agg(\n",
    "#         avg_p=(\"p_high\", \"mean\"),\n",
    "#         true_rate=(\"label\", \"mean\"),\n",
    "#         n=(\"label\", \"size\")\n",
    "#     ).reset_index()\n",
    "\n",
    "#     plt.figure(figsize=(7,5))\n",
    "#     plt.plot(bin_stats[\"avg_p\"], bin_stats[\"true_rate\"], marker=\"o\")\n",
    "#     plt.plot([0,1], [0,1], linestyle=\"--\")  # perfect calibration line\n",
    "#     plt.xlabel(\"Average predicted P(high erosion) in bin\")\n",
    "#     plt.ylabel(\"True bucket1 rate in bin\")\n",
    "#     plt.title(f\"Calibration / separation by probability bins {title}\")\n",
    "#     for _, r in bin_stats.iterrows():\n",
    "#         plt.text(r[\"avg_p\"], r[\"true_rate\"], str(int(r[\"n\"])), fontsize=9, ha=\"center\", va=\"bottom\")\n",
    "#     plt.show()\n",
    "\n",
    "#     return dfp, bin_stats\n",
    "\n",
    "# dfp_s1, bins_s1 = plot_probs_vs_buckets(eval_bucket_s1_df, proba_bucket_s1, title=\"(Scenario 1)\")\n",
    "# dfp_s2, bins_s2 = plot_probs_vs_buckets(eval_bucket_s2_df, proba_bucket_s2, title=\"(Scenario 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37a1f1",
   "metadata": {},
   "source": [
    "## Regressor Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_1 = []\n",
    "metrics_2 = []\n",
    "for train_df, eval_df_s1, eval_df_s2 in split_k_fold(df):\n",
    "    eval_df_s1, eval_df_s1_true = split_true(eval_df_s1, 0)\n",
    "    eval_df_s2, eval_df_s2_true = split_true(eval_df_s2, 6)\n",
    "\n",
    "    train_df = train_preprocessing(train_df, df_aux)\n",
    "\n",
    "    model_s1 = fit_model(train_df, 's1', verbose=False)\n",
    "    model_s2 = fit_model(train_df, 's2', verbose=False)\n",
    "\n",
    "    eval_pred_s1_df = predict(model_s1, eval_df_s1, 0)\n",
    "    eval_pred_s2_df = predict(model_s2, eval_df_s2, 6)\n",
    "\n",
    "    metric1_res = compute_metric1(eval_df_s1_true, eval_pred_s1_df, df_aux)\n",
    "    metric2_res = compute_metric2(eval_df_s2_true, eval_pred_s2_df, df_aux)\n",
    "\n",
    "    metrics_1.append(metric1_res)\n",
    "    metrics_2.append(metric2_res)\n",
    "\n",
    "    print('--------------------')\n",
    "    print(\"Scenario 1 metric:\", metric1_res)\n",
    "    print(\"Scenario 2 metric:\", metric2_res)\n",
    "\n",
    "print('---------------------------------')\n",
    "print(\"Mean Scenario 1 metric:\", np.mean(metrics_1))\n",
    "print(\"Mean Scenario 2 metric:\", np.mean(metrics_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5334f",
   "metadata": {},
   "source": [
    "## Classifier + Regressor Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f9e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_1 = []\n",
    "metrics_2 = []\n",
    "for train_df, eval_df_s1, eval_df_s2 in split_k_fold(df):\n",
    "    # Remove true values for safety\n",
    "    eval_df_s1, eval_df_s1_true = split_true(eval_df_s1, 0)\n",
    "    eval_df_s2, eval_df_s2_true = split_true(eval_df_s2, 6)\n",
    "\n",
    "    # Train the classifier and get the votes\n",
    "    train_bucket_s1_df = build_bucket_dataset(train_df, df_aux, \"s1\")\n",
    "    train_bucket_s2_df = build_bucket_dataset(train_df, df_aux, \"s2\")\n",
    "    bucket_model_s1 = fit_bucket_classifier(train_bucket_s1_df, \"s1\", verbose=False)\n",
    "    bucket_model_s2 = fit_bucket_classifier(train_bucket_s2_df, \"s2\", verbose=False)\n",
    "    votes_s1 = predict_probas(bucket_model_s1, eval_df_s1, df_aux, \"s1\")\n",
    "    votes_s2 = predict_probas(bucket_model_s2, eval_df_s2, df_aux, \"s2\")\n",
    "\n",
    "    # Prepare train set according to buckets\n",
    "    train_df = train_preprocessing(train_df, df_aux)\n",
    "    train_b1_df, train_b2_df = split_buckets(train_df, df_aux)\n",
    "\n",
    "    # Train regressors\n",
    "    model_b1_s1 = fit_model(train_b1_df, 's1', verbose=False)\n",
    "    model_b2_s1 = fit_model(train_b2_df, 's1', verbose=False)\n",
    "    model_b1_s2 = fit_model(train_b1_df, 's2', verbose=False)\n",
    "    model_b2_s2 = fit_model(train_b2_df, 's2', verbose=False)\n",
    "\n",
    "    eval_pred_s1_df = predict([model_b1_s1, model_b2_s1], eval_df_s1, 0, votes_s1)\n",
    "    eval_pred_s2_df = predict([model_b1_s2, model_b2_s2], eval_df_s2, 6, votes_s2)\n",
    "\n",
    "    metric1_res = compute_metric1(eval_df_s1_true, eval_pred_s1_df, df_aux)\n",
    "    metric2_res = compute_metric2(eval_df_s2_true, eval_pred_s2_df, df_aux)\n",
    "\n",
    "    metrics_1.append(metric1_res)\n",
    "    metrics_2.append(metric2_res)\n",
    "\n",
    "    print('--------------------')\n",
    "    print(\"Scenario 1 metric:\", metric1_res)\n",
    "    print(\"Scenario 2 metric:\", metric2_res)\n",
    "\n",
    "print('---------------------------------')\n",
    "print(\"Mean Scenario 1 metric:\", np.mean(metrics_1))\n",
    "print(\"Mean Scenario 2 metric:\", np.mean(metrics_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbc1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_bucket_s1_df, eval_bucket_s1_df = split_bucket_train_test(df_bucket_s1)\n",
    "# bucket_model_s1 = fit_bucket_classifier(train_bucket_s1_df, \"s1\", verbose=False)\n",
    "# proba_bucket_s1 = predict_bucket1_proba(bucket_model_s1, eval_bucket_s1_df, \"s1\")\n",
    "# pred_bucket_s1 = (proba_bucket_s1 > 0.5).astype(int)\n",
    "# print(\"Scenario 1 metrics:\", roc_auc_score(eval_bucket_s1_df[\"label\"].values, proba_bucket_s1), f1_score(eval_bucket_s1_df[\"label\"].values, pred_bucket_s1))\n",
    "\n",
    "# train_bucket_s2_df, eval_bucket_s2_df = split_bucket_train_test(df_bucket_s2)\n",
    "# bucket_model_s2 = fit_bucket_classifier(train_bucket_s2_df, \"s2\", verbose=False)\n",
    "# proba_bucket_s2 = predict_bucket1_proba(bucket_model_s2, eval_bucket_s2_df, \"s2\")\n",
    "# pred_bucket_s2 = (proba_bucket_s2 > 0.5).astype(int)\n",
    "# print(\"Scenario 2 metrics:\", roc_auc_score(eval_bucket_s2_df[\"label\"].values, proba_bucket_s2), f1_score(eval_bucket_s2_df[\"label\"].values, pred_bucket_s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp = bucket_model_s1.get_feature_importance()\n",
    "# fi = pd.DataFrame({\"feature\": bucket_model_s1.feature_names_, \"importance\": imp}).sort_values(\"importance\", ascending=False)\n",
    "# print(fi.head(20))\n",
    "\n",
    "# ths = np.linspace(0.001, 0.1)\n",
    "# best = None\n",
    "# for t in ths:\n",
    "#     pred_bucket_s1 = (proba_bucket_s1 > t).astype(int)\n",
    "#     f1 = f1_score(eval_bucket_s1_df[\"label\"].values, pred_bucket_s1)\n",
    "#     if best is None or f1 > best:\n",
    "#         best = f1\n",
    "#         best_t = t\n",
    "# print(\"Best threshold:\", best_t)\n",
    "# print(\"Best f1:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, eval_df_s1, eval_df_s2 = split_train_test(df)\n",
    "\n",
    "# eval_df_s1, eval_df_s1_true = split_true(eval_df_s1, 0)\n",
    "# eval_df_s2, eval_df_s2_true = split_true(eval_df_s2, 6)\n",
    "\n",
    "# train_df = train_preprocessing(train_df, df_aux)\n",
    "\n",
    "# model_s1 = fit_model(train_df, 's1')\n",
    "# model_s2 = fit_model(train_df, 's2')\n",
    "\n",
    "# eval_pred_df_s1 = predict(model_s1, eval_df_s1, 0)\n",
    "# eval_pred_df_s2 = predict(model_s2, eval_df_s2, 6)\n",
    "\n",
    "# print(\"Scenario 1 metric:\", compute_metric1(eval_df_s1_true, eval_pred_df_s1, df_aux))\n",
    "# print(\"Scenario 2 metric:\", compute_metric2(eval_df_s2_true, eval_pred_df_s2, df_aux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Regressor per bucket\n",
    "\n",
    "# train_b1_df, eval_b1_df_s1, eval_b1_df_s2, train_b2_df, eval_b2_df_s1, eval_b2_df_s2 = split_buckets(df, df_aux)\n",
    "\n",
    "# eval_b1_df_s1, eval_b1_df_s1_true = split_true(eval_b1_df_s1, 0)\n",
    "# eval_b1_df_s2, eval_b1_df_s2_true = split_true(eval_b1_df_s2, 6)\n",
    "\n",
    "# eval_b2_df_s1, eval_b2_df_s1_true = split_true(eval_b2_df_s1, 0)\n",
    "# eval_b2_df_s2, eval_b2_df_s2_true = split_true(eval_b2_df_s2, 6)\n",
    "\n",
    "# train_b1_df = train_preprocessing(train_b1_df, df_aux)\n",
    "# train_b2_df = train_preprocessing(train_b2_df, df_aux)\n",
    "\n",
    "# model_b1_s1 = fit_model(train_b1_df, 's1')\n",
    "# model_b1_s2 = fit_model(train_b1_df, 's2')\n",
    "# model_b2_s1 = fit_model(train_b2_df, 's1')\n",
    "# model_b2_s2 = fit_model(train_b2_df, 's2')\n",
    "\n",
    "# eval_pred_b1_df_s1 = predict(model_b1_s1, eval_b1_df_s1, 0)\n",
    "# eval_pred_b1_df_s2 = predict(model_b1_s2, eval_b1_df_s2, 6)\n",
    "# eval_pred_b2_df_s1 = predict(model_b2_s1, eval_b2_df_s1, 0)\n",
    "# eval_pred_b2_df_s2 = predict(model_b2_s2, eval_b2_df_s2, 6)\n",
    "\n",
    "# print(\"Bucket 1 Scenario 1 metric:\", compute_metric1(eval_b1_df_s1_true, eval_pred_b1_df_s1, df_aux))\n",
    "# print(\"Bucket 1 Scenario 2 metric:\", compute_metric2(eval_b1_df_s2_true, eval_pred_b1_df_s2, df_aux))\n",
    "# print(\"Bucket 2 Scenario 1 metric:\", compute_metric1(eval_b2_df_s1_true, eval_pred_b2_df_s1, df_aux))\n",
    "# print(\"Bucket 2 Scenario 2 metric:\", compute_metric2(eval_b2_df_s2_true, eval_pred_b2_df_s2, df_aux))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a3ed6",
   "metadata": {},
   "source": [
    "## Fit on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947beae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = df.copy()\n",
    "\n",
    "all_df = train_preprocessing(all_df, df_aux)\n",
    "\n",
    "model_s1 = fit_model(all_df, 's1')\n",
    "model_s2 = fit_model(all_df, 's2')\n",
    "\n",
    "# Save model\n",
    "latest_id = max([\n",
    "    int(f.removesuffix(\".model\").split(\"_\")[-1])\n",
    "    for f in os.listdir(\"models\") if f.endswith(\".model\")\n",
    "], default=-1)\n",
    "model_s1.save_model(f\"models/cb_s1_{latest_id + 1}.model\")\n",
    "model_s2.save_model(f\"models/cb_s2_{latest_id + 1}.model\")\n",
    "print(f\"Models saved to models/cb_s1_{latest_id + 1}.model and models/cb_s2_{latest_id + 1}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat submission\n",
    "t_df, t_sub = general_preprocessing(\n",
    "    pd.read_csv('data/test/df_volume_test.csv'),\n",
    "    pd.read_csv('data/test/df_generics_test.csv'),\n",
    "    pd.read_csv('data/test/df_medicine_info_test.csv'),\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "t_df_s1, t_df_s2 = split_scenario(t_df)\n",
    "t_pred_df_s1 = predict(model_s1, t_df_s1, 0)\n",
    "t_pred_df_s2 = predict(model_s2, t_df_s2, 6)\n",
    "\n",
    "t_pred = pd.concat([t_pred_df_s1, t_pred_df_s2])\n",
    "t_pred = t_pred[['country', 'brand_name', 'months_postgx', 'volume']]\n",
    "\n",
    "t_final = t_sub.merge(t_pred, on=[\"country\", \"brand_name\", \"months_postgx\"], how=\"left\", validate=\"one_to_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279027a8",
   "metadata": {},
   "source": [
    "## Fit Reg + Cls on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = df.copy()\n",
    "\n",
    "# Train the classifier\n",
    "all_bucket_s1_df = build_bucket_dataset(all_df, df_aux, \"s1\")\n",
    "all_bucket_s2_df = build_bucket_dataset(all_df, df_aux, \"s2\")\n",
    "bucket_model_s1 = fit_bucket_classifier(all_bucket_s1_df, \"s1\", verbose=False)\n",
    "bucket_model_s2 = fit_bucket_classifier(all_bucket_s2_df, \"s2\", verbose=False)\n",
    "\n",
    "# Prepare train set according to buckets\n",
    "all_df = train_preprocessing(all_df, df_aux)\n",
    "all_b1_df, all_b2_df = split_buckets(all_df, df_aux)\n",
    "\n",
    "# Train regressors\n",
    "model_b1_s1 = fit_model(all_b1_df, 's1', verbose=False)\n",
    "model_b2_s1 = fit_model(all_b2_df, 's1', verbose=False)\n",
    "model_b1_s2 = fit_model(all_b1_df, 's2', verbose=False)\n",
    "model_b2_s2 = fit_model(all_b2_df, 's2', verbose=False)\n",
    "\n",
    "# Save model\n",
    "latest_id = max([\n",
    "    int(f.removesuffix(\".model\").split(\"_\")[-1])\n",
    "    for f in os.listdir(\"models\") if f.endswith(\".model\")\n",
    "], default=-1)\n",
    "model_b1_s1.save_model(f\"models/cb_b1_s1_{latest_id + 1}.model\")\n",
    "model_b2_s1.save_model(f\"models/cb_b2_s1_{latest_id + 1}.model\")\n",
    "model_b1_s2.save_model(f\"models/cb_b1_s2_{latest_id + 1}.model\")\n",
    "model_b2_s2.save_model(f\"models/cb_b2_s2_{latest_id + 1}.model\")\n",
    "bucket_model_s1.save_model(f\"models/cb_bucket_s1_{latest_id + 1}.model\")\n",
    "bucket_model_s2.save_model(f\"models/cb_bucket_s2_{latest_id + 1}.model\")\n",
    "print(\"Models saved to:\",\n",
    "      f\"models/cb_b1_s1_{latest_id + 1}.model\",\n",
    "      f\"models/cb_b2_s1_{latest_id + 1}.model\",\n",
    "      f\"models/cb_b1_s2_{latest_id + 1}.model\",\n",
    "      f\"models/cb_b2_s2_{latest_id + 1}.model\",\n",
    "      f\"models/cb_bucket_s1_{latest_id + 1}.model\",\n",
    "      f\"models/cb_bucket_s2_{latest_id + 1}.model\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03227315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat submission\n",
    "t_df, t_sub = general_preprocessing(\n",
    "    pd.read_csv('data/test/df_volume_test.csv'),\n",
    "    pd.read_csv('data/test/df_generics_test.csv'),\n",
    "    pd.read_csv('data/test/df_medicine_info_test.csv'),\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "t_df_s1, t_df_s2 = split_scenario(t_df)\n",
    "votes_s1 = predict_probas(bucket_model_s1, t_df_s1, df_aux, \"s1\")\n",
    "votes_s2 = predict_probas(bucket_model_s2, t_df_s2, df_aux, \"s2\")\n",
    "t_pred_s1_df = predict([model_b1_s1, model_b2_s1], t_df_s1, 0, votes_s1)\n",
    "t_pred_s2_df = predict([model_b1_s2, model_b2_s2], t_df_s2, 6, votes_s2)\n",
    "\n",
    "t_pred = pd.concat([t_pred_s1_df, t_pred_s2_df])\n",
    "t_pred = t_pred[['country', 'brand_name', 'months_postgx', 'volume']]\n",
    "\n",
    "t_final = t_sub.merge(t_pred, on=[\"country\", \"brand_name\", \"months_postgx\"], how=\"left\", validate=\"one_to_one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d4e11",
   "metadata": {},
   "source": [
    "## Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_id = max([\n",
    "    int(f.removeprefix(\"submission\").removesuffix(\".csv\")) for f in\n",
    "    os.listdir(\"submissions/\") if f.startswith(\"submission\")\n",
    "], default=-1)\n",
    "t_final.to_csv(f\"submissions/submission{latest_id + 1}.csv\", index=False)\n",
    "print(f\"Saved to submissions/submission{latest_id + 1}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
